{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pprint\n",
    "import math\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from string import punctuation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from os import listdir\n",
    "import re\n",
    "\n",
    "#stop words\n",
    "class Multinomial_Naive_Bayes:\n",
    "\n",
    "    def fit(self,df,out):\n",
    "        self.feature_set = df.columns.values\n",
    "        dim = df.shape\n",
    "\n",
    "        initial_size = dim[0]\n",
    "        size = dim[1]\n",
    "\n",
    "        df['_output'] = out\n",
    "        classes = df[\"_output\"].unique()\n",
    "\n",
    "        dty = {}\n",
    "        # columns = df.columns.values\n",
    "\n",
    "        for uni in classes:\n",
    "            dty[uni] = {}\n",
    "            data = df[df['_output'] == uni].copy()[self.feature_set]\n",
    "\n",
    "            class_size = data.shape[0]\n",
    "            data.loc['total'] = data.sum()\n",
    "\n",
    "            prob = class_size/initial_size\n",
    "\n",
    "            denom = data.loc['total'].sum()\n",
    "\n",
    "\n",
    "            for c in self.feature_set:\n",
    "\n",
    "                sum = data.loc['total',c]\n",
    "\n",
    "                if sum==0:\n",
    "                    sum = (sum + 1) / (denom + size)\n",
    "                else:\n",
    "                    sum = sum / denom\n",
    "\n",
    "                dty[uni][c] = sum\n",
    "\n",
    "            # prob = round(prob,2)\n",
    "\n",
    "            dty[uni]['_probability']= prob\n",
    "            # dty[uni]['prob']['probability'] = prob\n",
    "\n",
    "\n",
    "        self.train_data = dty\n",
    "\n",
    "        return dty\n",
    "\n",
    "    def predict(self,df):\n",
    "        out = []\n",
    "\n",
    "        for ind,row in df.iterrows():\n",
    "            y_pred = self.predict_row(row)\n",
    "            out.append(y_pred)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def predict_row(self,df):\n",
    "\n",
    "        features = list(df.keys())\n",
    "        features = set(self.feature_set).intersection(set(features))\n",
    "\n",
    "        classes = []\n",
    "\n",
    "        for key in self.train_data.keys():\n",
    "            log_prob = 1\n",
    "            for f in features:\n",
    "                log_prob *= self.train_data[key][f]\n",
    "\n",
    "            log_prob *= self.train_data[key]['_probability']\n",
    "            row = (key,round(log_prob,2))\n",
    "            classes.append(row)\n",
    "\n",
    "        classes = pd.DataFrame(classes,columns=['class','probability']).sort_values('probability', ascending=[False])\n",
    "        classes.index = range(len(classes.index))\n",
    "\n",
    "        return classes.loc[0,'class']\n",
    "\n",
    "\n",
    "\n",
    "def read_file(dir):\n",
    "        stop_words=[\"a\",\"the\",\"able\",\"about\",\"data\", \"about\", \"above\",\n",
    "                    \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\",\n",
    "                    \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\",\n",
    "                    \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\n",
    "                    \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\n",
    "                    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\",\n",
    "                    \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\",\n",
    "                    \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\",\n",
    "                    \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\",\n",
    "                    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\",\n",
    "                    \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\",\n",
    "                    \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\",\n",
    "                    \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\",\n",
    "                    \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\",\n",
    "                    \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\",\n",
    "                    \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\",\n",
    "                    \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\",\n",
    "                    \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\",\n",
    "                    \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\",\n",
    "                    \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\",\n",
    "                    \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\",\n",
    "                    \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\",\n",
    "                    \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\",\n",
    "                    \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\",\n",
    "                    \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "                    \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\",\n",
    "                    \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]\n",
    "\n",
    "        #add to the dictionary the word which is not in stopwords\n",
    "        ls =listdir(dir)\n",
    "        dict_features = {}\n",
    "        dict_record = []\n",
    "        output_classes = []\n",
    "\n",
    "        for folder in ls:\n",
    "            files=listdir(dir+folder)\n",
    "\n",
    "            for file in files:\n",
    "                output_classes.append(folder)\n",
    "                feature_dict = {}\n",
    "\n",
    "                feature_dict['_out_class'] = folder\n",
    "\n",
    "                f=open(dir+folder+'/'+file,'r',encoding=\"ISO-8859-1\")\n",
    "                text = f.read()\n",
    "\n",
    "                tokens = re.compile('\\w+').findall(text)\n",
    "                # words = [token.lower() for token in tokens if ( token.isalpha() and len(token)>3 and token not in stop_words)]\n",
    "\n",
    "                feature_dict['_file'] = file\n",
    "\n",
    "                for token in tokens:\n",
    "                    if token.isalpha() and len(token)>3 and token not in stop_words:\n",
    "                        token.lower()\n",
    "\n",
    "                        if token in feature_dict:\n",
    "                            feature_dict[token] += 1\n",
    "                        else:\n",
    "                            feature_dict[token] = 1\n",
    "\n",
    "                        if token in dict_features:\n",
    "                            dict_features[token] += 1\n",
    "                        else:\n",
    "                            dict_features[token] = 1\n",
    "\n",
    "                dict_record.append(feature_dict)\n",
    "\n",
    "\n",
    "        a=sorted(dict_features.items(), key=operator.itemgetter(1), reverse=True)[:3000]\n",
    "        s=a[:3000]\n",
    "\n",
    "        # frequency=[i[1] for i in a]\n",
    "        words=[i[0] for i in a]\n",
    "\n",
    "        lgt = len(dict_record)\n",
    "        data = np.zeros(shape=(lgt,len(words)))\n",
    "\n",
    "\n",
    "        for i in range(lgt):\n",
    "            dty = dict_record[i]\n",
    "            # output_classes.append(dty['_out_class'])\n",
    "            dty_keys = set(dty.keys()).intersection(set(words))\n",
    "            for k in dty_keys:\n",
    "                ind = words.index(k)\n",
    "                data[i,ind] = dty[k]\n",
    "            dict_record[i] = dty['_file']\n",
    "\n",
    "        df = pd.DataFrame(data,columns=words)\n",
    "\n",
    "        return df,np.array(output_classes)\n",
    "\n",
    "def compare(y_true,y_pred):\n",
    "    con_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(\"Confusion Matrix For Manual Naive Bayes\")\n",
    "    print(con_mat)\n",
    "    print()\n",
    "\n",
    "    print(\"Classification Report For Testing Data In Manual Naive Bayes\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "def custom_MultinomialNB(df,out):\n",
    "    custom_clf = Multinomial_Naive_Bayes()\n",
    "\n",
    "    features = df.columns.values\n",
    "    data = df.copy().values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, out, random_state=1)\n",
    "\n",
    "    X_train = pd.DataFrame(X_train,columns=features)\n",
    "    dty = custom_clf.fit(X_train,y_train)\n",
    "    # pprint.pprint(dty)\n",
    "\n",
    "    X_test = pd.DataFrame(X_test,columns=custom_clf.feature_set)\n",
    "    y_pred = custom_clf.predict(X_test)\n",
    "\n",
    "\n",
    "    print(\"Predicted Output For Testing Data In Manual Naive Bayes\")\n",
    "    print(y_pred)\n",
    "    print()\n",
    "\n",
    "    return y_test,y_pred\n",
    "\n",
    "def scikit_NB(data,out):\n",
    "    clf = MultinomialNB()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, out, random_state=1)\n",
    "\n",
    "    clf.fit(x_train,y_train)\n",
    "\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "\n",
    "    print(\"Predicted Output For Testing Data In Sklearn Naive Bayes\")\n",
    "    print(y_pred)\n",
    "    print()\n",
    "\n",
    "    return y_test,y_pred\n",
    "\n",
    "df,out = read_file('C:/Users/Rohith/Downloads/20_newsgroups/')\n",
    "print(df)\n",
    "\n",
    "ytest,ypred = custom_MultinomialNB(df,out)\n",
    "compare(ytest,ypred)\n",
    "\n",
    "data = df.copy().values\n",
    "\n",
    "ytest,ypred = scikit_NB(data,out)\n",
    "compare(ytest,ypred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
